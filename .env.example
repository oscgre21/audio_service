# RabbitMQ Configuration
RABBITMQ_HOST=rabbit.oscgre.com
RABBITMQ_PORT=5672
RABBITMQ_USERNAME=admin
RABBITMQ_PASSWORD=your_password_here
RABBITMQ_VHOST=/
RABBITMQ_EXCHANGE=aloud_exchange
RABBITMQ_QUEUE_PREFIX=aloud_

# Audio Configuration
AUDIO_MODEL_PATH=bosonai/higgs-audio-v2-generation-3B-base
AUDIO_TOKENIZER_PATH=bosonai/higgs-audio-v2-tokenizer
AUDIO_OUTPUT_DIR=generated_audio
AUDIO_TEMP_DIR=generated_audio/temp_chunks
AUDIO_MAX_CHUNK_LENGTH=566
AUDIO_TORCH_SEED=42
AUDIO_MAX_NEW_TOKENS=1024
AUDIO_TEMPERATURE=0.3
AUDIO_TOP_P=0.95
AUDIO_TOP_K=50
AUDIO_OUTPUT_FORMAT=mp3                # Output format: wav or mp3
AUDIO_MP3_BITRATE=128k                 # MP3 bitrate: 64k, 128k, 192k, 256k, 320k

# Upload Configuration
UPLOAD_URL=http://localhost:3000/audio/upload
SPEECH_UPLOAD_URL=http://localhost:3000/audio/speech/upload
WORD_UPLOAD_URL=http://localhost:3000/audio/speech/upload
SENTENCE_UPLOAD_URL=http://localhost:3000/audio/sentence/upload
CUSTOM_UPLOAD_URL=http://localhost:3000/audio/custom/upload
UPLOAD_TOKEN=your_jwt_token_here
UPLOAD_MAX_RETRIES=3
UPLOAD_RETRY_DELAY=2
UPLOAD_ENABLED=false
UPLOAD_TIMEOUT=30

# App Configuration
APP_NAME=higgs-audio-service
ENVIRONMENT=development
LOG_LEVEL=INFO
LOG_FILE=audio_processing.log

# Processing Queue Configuration
QUEUE_MAX_SIZE=1000                    # Maximum messages in internal queue (0 = unlimited)
CONCURRENT_PROCESSORS=3                # Number of concurrent message processors

# Validation Strategy Configuration
VALIDATION_MIN_TEXT_LENGTH=1           # Minimum text length in characters
VALIDATION_MAX_TEXT_LENGTH=10000       # Maximum text length in characters

# Post-Processing Strategy Configuration
ENABLE_NOTIFICATIONS=true              # Enable notification sending
ENABLE_ANALYTICS=true                  # Enable analytics recording
ENABLE_CLEANUP=false                   # Enable temp file cleanup
WEBHOOK_URL=                          # Optional webhook URL for completion events

# Transcription Configuration
TRANSCRIPTION_ENABLED=true             # Enable transcription processing
TRANSCRIPTION_MODEL=base               # Model size: tiny, base, small, medium, large
TRANSCRIPTION_DEVICE=cpu               # Device: cuda, cpu
TRANSCRIPTION_COMPUTE_TYPE=int8        # Compute type: float16, int8
TRANSCRIPTION_BATCH_SIZE=16            # Batch size for transcription

# Authentication Configuration
AUTH_URL=http://localhost:3000/auth/login  # Authentication endpoint URL
AUTH_EMAIL=admin@test.com                  # Email for authentication
AUTH_PASSWORD=password123                  # Password for authentication
AUTH_TOKEN_REFRESH_MARGIN=300              # Seconds before expiry to refresh token (5 min)

# LLM Provider Configuration
LLM_PROVIDER=ollama                        # LLM provider: ollama, claude, openai
LLM_BASE_URL=https://llm.oscgre.com        # Base URL for LLM API
LLM_API_KEY=                               # API key for providers that require it
LLM_MODEL=llama3.1                         # Default model to use
LLM_TEMPERATURE=0.7                        # Temperature for generation (0.0-1.0)
LLM_MAX_TOKENS=2000                        # Maximum tokens to generate
LLM_TIMEOUT=30                             # Timeout in seconds for LLM requests

# Ollama Specific Configuration
OLLAMA_MODEL=llama3.1                      # Ollama model name

# Claude Specific Configuration
CLAUDE_API_KEY=                            # Claude API key
CLAUDE_MODEL=claude-3-opus-20240229        # Claude model version

# OpenAI Specific Configuration
OPENAI_API_KEY=                            # OpenAI API key
OPENAI_MODEL=gpt-4-turbo-preview           # OpenAI model name

# Word Processing Strategy Configuration
WORD_PROCESSING_ENABLED=true               # Enable word-by-word processing
WORD_PROCESSING_MAX_CONCURRENT=5           # Maximum concurrent word processing
WORD_PROCESSING_TIMEOUT=30.0               # Timeout in seconds per word